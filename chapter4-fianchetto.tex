\chapter{Fianchetto: Accelerating Data Motion Across the Board}
\label{chap:fianchetto}

\newcommand{\dmx}[0]{Fianchetto\xspace}
\newcommand{\arch}[0]{DRX\xspace}
\newcommand{\systemName}[0]{\emph{Fianchetto}\xspace}
\newcommand{\drx}[0]{DRX\xspace}
\newcommand{\drxs}[0]{DRXs\xspace}

\newcommand{\vs}[0]{\textsf{Video Surveillance}\xspace}
\newcommand{\sd}[0]{\textsf{Sound Detection}\xspace}
\newcommand{\bs}[0]{\textsf{Brain Stimulation}\xspace}
\newcommand{\pir}[0]{\textsf{Personal Info Redaction}\xspace}
\newcommand{\dhj}[0]{\textsf{Database Hash Join}\xspace}

\section{Introduction}
\label{fianchetto:sec:intro}
%\vspace{-2ex}

With the effective end of Dennard Scaling~\cite{dennard_scaling}, the dark silicon~\cite{dark_silicon_isca2011, dark_silicon:babak} phenomenon has led to the development and adoption of Domain-Specific Architectures (DSA) or accelerators.
%
With the Cambrian explosion of accelerators~\cite{diannao:asplos:2014, dnnoptimizing:fpga:2015, pudiannao:asplos:2015, shidiannao:isca:2015, cambricon:isca:2016, cambricon-x:micro:2016, cbrain:dac:2016, dnnweaver:micro:2016, fusedlayercnn:micro:2016, tabla:hpca:2016, escher:fccm:2017, pipelayer:hpca:2017, scnn:isca:2017, tpu:isca:2017, yongming-isca17, maeri:asplos18, unpu:isscc:2018, eyerissv2:journal:2019, simba:micro:2019, tangram:asplos19, awb-gcn:micro:2020, hygcn:hpca:2020,planaria:micro:2020, sigma:hpca:2020, engn:toc:2021, gcnax:hpca:2021, graphicionado:micro:2016, extrav:pvldb:2017, accugraph:pact:2018, hats:micro:2018, graphp:hpca:2018, graphr:hpca:2018, minnow:asplos:2018, conda:isca:2019, phi:micro:2019, graphpulse:micro:2020, deepgraph:hpca:2021, jetstream:micro:2021, lccg:sc:2021, darwin:asplos:2018, genax:isca:2018, smem:fpl:2018, asap:toc:2019, gencache:micro:2019, medal:micro:2019, genasm:micro:2020, geniehd:date:2020, nest:iccad:2020, savi:iccad:2020, seedex:micro:2020, wfa:fpl:2021, genstore:asplos:2022, segram:isca:2022, meet-the-walker:micro:2013,murray:micro:2016,robox:isca:2018,pointacc:micro:2021,robomorphic:asplos:2021}, it is fitting to consider the current cadence of the architecture design as the golden age of accelerators.  
%
Amazon Web Service (AWS)~\cite{aws-inferentia:2019, aws-trainium:2022}, Microsoft Azure~\cite{catapult:isca:2014, cloud-scale-acc:micro:2016, brainwave:isca:2018, microsoft-azure:zipline:2019}, and Google Cloud Platform (GCP)~\cite{tpu:isca:2017,tpuv4i:isca:2021,google-vcu:asplos:2021} as the three providers of public cloud recently started offering accelerator equipped instances. 
%

The offering is the result of market push toward compute-intensive applications such as genomics, content streaming, recommendation systems, virtual reality, data analytics, etc. 
%
%\stingw{I know heres a transition to multiple domains and multiple DSAs here. But "Such applications" look like single-domain on the word. At least I feel like machine learning is a single domain. \malian{} what do you think?}
%
Such applications often cross the boundary of multiple domains, each of which can be potentially accelerated with its own domain-specific architecture (DSA). 
%
These applications would maximally benefit from the DSAs in the cloud only if all the domains are accelerated and not just one. 
%

%
\begin{figure}[t]
    \centering
    \begin{subfigure}[b]{\columnwidth}
    \includegraphics[width=\textwidth]{figure/fianchetto/cpu-overview.pdf}
    \caption{Current multi-accelerator systems.}
    \label{fig:overview:current}
    \end{subfigure}
    %
    \hspace{0.5in}
    \begin{subfigure}[b]{\columnwidth}
    \includegraphics[width=\textwidth]{figure/fianchetto/dmx-overview.pdf}
    \caption{Multi-accelerator systems with \dmx.}
    \label{fig:overview:dmx}
    \end{subfigure}
    %
    \caption{Current multi-acceleration systems rely on CPU for accelerator chaining. (a) shows a system with four heterogeneous accelerator cards. The CPU needs to intervene in the communication between accelerator cards. This involves data copies from system memory to accelerator memory and non-trivial data transformations. (b) The proposed \dmx framework removes the CPU from the data path of multi-acceleration. \dmx delivers the performance of a monolithic accelerator while offering the composability and programmablity of the baseline system.}
    \vspace{-3ex}
    
    \label{fig:overview}
    \end{figure}

To enable heterogeneous cross-domain multi-acceleration, there is an essential need for cross-stack solutions for accelerator chaining to enable intimate communication between different DSAs, each of which is responsible for accelerating a part of a single application. 
%
This paper sets out to explore a heterogeneous cross-domain multi-acceleration datacenter that harvests the recent initiative towards democratizing hardware design and enables the vision of a \textit{sea of accelerators}~\cite{pymtl-2020,taylor-basejump,BlackParrot,cong-democratizing, gonzalez-2023-profiling}.

In a cross-domain multi-acceleration system, a chain of DSAs is created, where each DSA accepts inputs in a specific data structure and produces outputs in another data structure. 
%
The accelerator chaining currently needs to involve the system CPU (Figure~\ref{fig:overview}(a)) for restructuring and then exchanging data between different DSAs to run a single application.
%
This restructuring usually involves reshaping and reformatting the output of one DSA to match the input of the next.
%

We refer to the data restructuring and communication overhead of executing a single application using a number of different DSA as \textit{data motion} overhead. % in multi-accelerator systems.
%
Using the CPU for data motion requires frequent copies between the host and the DSA memory. 
%
Moreover, because the overhead of data restructuring between DSAs exacerbates with the number of accelerators, the CPU quickly becomes the performance bottleneck at scale.

To address these challenges, we propose Data Motion Acceleration (\dmx) as a solution that integrates a programmable Data Restructuring Accelerator (\drx) with each DSA.
%
\drx offloads data restructuring computation from the CPU back to a specialized engine near the DSAs.
%
\dmx illustrated in Figure~\ref{fig:overview}(b) removes the CPU from the data path of accelerator chaining and gives the illusion of a monolithic but composable accelerator to the user application. 

\dmx offloads the data restructuring operations to a scale-out programmable accelerator (\drx) while running the control plane on the CPU. 
%
\drx acts as a compute-enabled interface through which data moves between DSAs while \drx itself encapsulates a domain-specific accelerator. Although there have been efforts in offloading ser/des protocols to hardware~\cite{optimusprime:asplos:2020,karandikar-2021-protobuf}, prior work has not considered acceleration and offloading of cross-domain DSA communication, which enables efficient and seamless accelerator chaining.

We evaluate \dmx using five end-to-end applications, each of which comprised of kernels from different domains weaved together using data restructuring  kernels. 
%
%We run these applications on heterogeneous accelerator cards connected through PCIe lanes to the CPU.
%
We evaluate the scalability, performance, and energy of various \dmx configurations with a baseline that uses the same accelerator but still executes data restructuring on the host CPU.
%
DMX provides on average 3.4$\times$ to 8.2$\times$ speedup on end-to-end latency, 3.0$\times$ to 13.6$\times$ improvements on throughput, and 3.8$\times$ to 5.2$\times$ improvements on energy consumption.
%
The significant additional improvements over a baseline that itself maximally speedups an application using multiple DSAs show the emerging importance of data motion and restructuring as accelerators take the stage in datacenters.
