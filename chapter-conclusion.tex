\chapter{Conclusion}
\label{conclusion:chap}
%
The dissertation investigates the redundant communication between servers for large-scale web and cache requests and redundant data movement between accelerators for compute-intensive applications.
%
The redundancy is an impending and critical issue for datacenters designed for hardware accelerators and disaggregated resources.
% 
The dissertation makes the following three contributions to address this.


The first contribution of the dissertation is \daronpon. 
%
\daronpon is a datacenter-wide, inter-rack distributed load-balancing system for replicated datacenter services.  
%
\daronpon, at its heart, is a highly efficient gossip protocol that ensures that requests originating at any point in the datacenter can be directed away from overloaded replicas at sub-RTT timescales.
%
Through a mixture of simulation and deployment within AWS's cloud network, we show that \daronpon reduces the 99th percentile of latency for common workloads by up to 2$\times$ while admitting 10\% more requests per unit time. 

The second contribution of the dissertation is \dmx. 
%
\dmx acts as a compute-enabled bypass for inter-accelerator communication. 
%
The data restructuring and communication overhead of executing a single application using a chain of accelerators is defined as the data motion overhead.
%
With the current paradigm of using accelerators, the data motion overhead is very likely to outweigh the benefits from all these chained heterogeneous accelerators.
%
In contrast to previous works on accelerators that deal with accelerating only compute kernels, \dmx focuses on accelerating data motion within a chain of heterogeneous accelerators in a multi-accelerator datacenter.
%
To that end, \dmx reduces data movement, accelerates data restructuring, and enables interoperability between heterogeneous accelerators from different domains through a cross-stack hardware-software solution. 
%
The results from five end-to-end applications show that utilizing \dmx offers up to 8.2$\times$, 13.6$\times$, and 5.2$\times$ improvement in latency, throughput, and energy efficiency in a multi-accelerator system, respectively.

The third contribution of the dissertation is \aurelia. 
%%%%%
\aurelia leverages the emerging interconnect of CXL to investigate the design of a scalable fabric for accelerators and fabric-attached memory expansion.
%
Compute Express Link (CXL) has emerged as a frontier for disaggregation by providing a fabric supporting memory accessing, caching, and peer-to-peer memory access between devices.
%
CXL externalizes the internal memory fabric of a server and blurs the notion of server for realistic disaggregation.
%
The key feature of enabling CXL as a memory fabric is its support of multi-level switching up to 4096 endpoints.
%
However, CXL's current multi-level switching poses challenges on scalability and latency. 
%
To cope with the expected scale of CXL fabric and take full advantage of disaggregation, we propose \aurelia.
%
\aurelia architects addressing, routing, and transport as networking layers, which are typical in host networking, for CXL fabric.
%
\aurelia uses existing CXL mechanisms to realize the much-needed functionalities for a scalable fabric.
%
With these networking layers, \aurelia improves by 27\% on throughput for large language model inference and up to 2.6 $\times$ for key-value stores on YCSB benchmarks.