\chapter{Introduction}

%\TODO{Make it fully 2-page with some rewriting and minor extension on details}
% Use Luiz Andr√© Barros' warehouse-scale computer arguments
Modern datacenters are warehouse-scale computers~\cite{datacenter-as-a-computer:2013}.
%
These warehouse-scale computers serve as the foundation of cloud computing, e.g. IaaS, SaaS, and serverless.
%
They host commodity servers equipped with many general-purposed CPU cores.
%
The servers are interconnected with 100 Gbps or faster network connections.
%
They are clustered into fleets for different functions. For example, one serve the web requests, another one serve as data storage, and the other one serve as in-memory cache for frequently accessed objects.
%
Communication and data movement between servers are frequent and demonstrate diverse patterns depends on the mix of traffic from different functions 
~\cite{nishtala_memcached_nsdi13, google-datacenter-isca15, facebook_datacenter:hpca:2018,azure_serverless_computing:2021}.
%\TODO{Cite Google, Meta and Azure's paper on workload and traces}

Moreover, hardware accelerators are introduced to the datacenters given the rise of compute-intensive applications. 
%trend of deploying large machine learning models, 
%
These accelerators include GPUs, TPUs, and video codec accelerators etc., deployed in the production datacenters nowadays~\cite{tpu:isca:2017, google-vcu:asplos:2021, google-datacenter-isca15, aws-trainium:2022,aws-inferentia:2019}.
% the increased footprint of non-CPU, accelerator hardware (GPU included as the general matrix accelerator and beyond).
% cloud datacenters are equipped with accelerators, ranging from generic GPUs, FPGAs, 
% to more workload-specific ASICs
%
The introduction of these accelerators demonstrates a monumental shift towards heterogeneous hardware landscape beyond a massive number of homogenous CPU cores.   
%
The accelerators accelerate specific compute-intensive workloads, such as video encoding and decoding, scientific computation, and large-scale machine learning applications.
%
% How do we connect hardware accelerators to the data movement?
They are integrated with the system and operate on data moved from the host memory and return the results back to it. 
%
Thus, efficient data movement ensures that these dedicated accelerators are well-utilized.

In addition to the heterogeneous hardware landscape, resource disaggregation aims at efficient resource provision in the datacenters.
%  
Resource disaggregation allocates resources, such as CPU cores, memory capacity, and storage capacity, located on different physical machines in a logically unified manners~\cite{
legoos:osdi:2018, far-memory:eurosys:2020, leap:atc:2020,aifm:osdi:2020,carbink:osdi:2022,hydra:fast:2022}.
% \TODO{Cite LegoOS and other 10 papers here. These citations should be already in the CXL paper}. 
%especially memory disaggregation.
%
The rationale behind disaggregation is to satisfy users with diverse requests on different resources.
%
%\TODO{How do we connect disaggregation to increase data movement?}
Disaggregation, while aiming at efficient provision of resources, exposes the communication and data movement between CPU and memory/devices to a network fabric connecting the resources~\cite{kona:asplos:2021, intel-cxl:ieee-micro:2023, tpp:asplos:2023, pond:asplos:2023, aurelia:words:2023}.   
%
The externalized communication and data movement that used to confined within a server stemming from disaggregation motivates the study of this thesis. 
%with the help of RDMA  and potentially CXL.

%A routable memory fabric connecting accelerators and disaggregated memory modules stands in the center of this issue and offers the benefit of 
%
%Redundant communication and data movement decreases the utilization of resources. Idled resources
%leave performance gain on the table.
%
%\stingw{the point of connections between accelerators and memory disaggregation is the memory fabric}
%
%\stingw{the large scale web/cache requests in datacenters can also be benefit from the memory disaggregation.}

%
The current datacenters demonstrate diverse communication and data movement patterns on different scale with their corresponding applications.
% 
We are, in particular, interested in the data movement of the following scenarios:
\begin{enumerate}[noitemsep]
    \item RPC communication for web and cache applications. The data movement is on the scale of a few packets to a few MBs in total size.
    \item Compute-intensive applications with the use of non cache-coherent accelerators. The data movement is with chunks of data on the scale up to 100s of MBs.
    \item Key-value store on disaggregated memory modules. The data movement operates on cacheline granularity to KBs in total size.  
\end{enumerate} 
% (1) RPC communication for web and cache applications. The data movement is on the scale of a few packets to a few MBs in total size.
% %
% (2) Compute-intensive applications with the use of non cache-coherent accelerators. The data movement is with chunks of data on the scale up to 100s of MBs.
% %
% (3) Key-value store on disaggregated memory modules. The data movement operates on cacheline granularity to KBs in total size.  
%
In short, the current and conventional design of control and data plane shows redundant communication and data movement between servers for large scale web/cache requests and between accelerators for compute-intensive applications.
%
We argue that this redundancy is an impending and critical issue for datacenters designed for hardware accelerator and disaggregated resources. 
%
This thesis proposes to intelligently route the communication and data movement to bypass the congested or redundantly detoured path with a minimal addition of control logic. 
%
% The commonality shared by these seemingly different problems is that how we reason about 
% the communication/data movement pattern and provide systemic design and strategy for the problems.




%Data centers are the home of the cloud computing we know today.
%
